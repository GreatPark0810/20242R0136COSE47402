{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e4df5a",
   "metadata": {},
   "source": [
    "# Discussion and Takeaway"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891b970f",
   "metadata": {},
   "source": [
    "## 2.1. Data Manipulation\n",
    "\n",
    "### 2.1.5. Saving Memory\n",
    "To prevent memory waste, we should use slice notation [:] rather than just using variables!  \n",
    "\n",
    "For example,  \n",
    "A = A + B (X)  \n",
    "A[:] += A + B (O)  \n",
    "\n",
    "## 2.2. Data Preprocessing  \n",
    "\n",
    "### 2.2.2. Data Preparation\n",
    "We can get rid of NaN values, which might be dangerous when we use data, by introducing some strategies(RoofType_Slate, RoofType_nan, or numerical values)!  \n",
    "\n",
    "## 2.3. Linear Algebra\n",
    "\n",
    "### 2.3.5. Basic Properties of Tensor Arithmetics  \n",
    "We should not confuse * operator with matrix multiplication in tensor arithmetics! * operator is sort of scalar product...\n",
    "\n",
    "### 2.3.6. Reduction  \n",
    "axis=0 is along columns, on the other hand, axis=1 is along rows!\n",
    "\n",
    "### 2.3.8. Dot Product\n",
    "\\* operator can be considered as dot product in PyTorch!\n",
    "\n",
    "### 2.3.9. Matrix-Vector Products\n",
    "@ operator can be considered as matrix multiplication in PyTorch!\n",
    "\n",
    "### 2.3.11. Norms\n",
    "Sort of norms can be distinguished to three main norms!\n",
    "1. torch.norm() (Vector) : Euclidean norm\n",
    "2. abs().sum() : Manhattan distance\n",
    "3. torch.norm() (Matrix) : Frobenius norm\n",
    "\n",
    "## 2.5. Automatic Differentiation\n",
    "\n",
    "### 2.5.1. A Simple Function\n",
    "- If y is scalar function of vector x, we can get gradient of y(x.grad) by processing y.backward().\n",
    "- If we want to reset gradient of y, we can run x.grad.zero_().\n",
    "\n",
    "### 2.5.2. Backward for Non-Scalar Variables\n",
    "- If y is not a scalar but a vector, we can earn Jacobian derivatives.\n",
    "- However, we generally get summing up the gradients of each component of y, w.r.t. vector x.\n",
    "\n",
    "### 2.5.3. Detaching Computation\n",
    "- If we want to use value of vector x to express other variables but not want to consider those variables as function of x, we can copy the value of x to other new variables, where we can avoid to be differentiated by x, by using detach().\n",
    "\n",
    "## 3.1. Linear Regression\n",
    "\n",
    "### 3.1.1. Basics\n",
    "- Minibatch SGD method is widely used in Deep Learning, but why? Quasi-Newton method might do better performance?\n",
    "- Maybe the local minimum problem cannot be dealt with in Quasi-Newton, while SGD can be...\n",
    "\n",
    "### 3.1.2. Vectorization for Speed\n",
    "- In deep learning, especially training phase, we should use vectorization method rather than for-loop to utilize running time!\n",
    "\n",
    "## 3.2. Object-Oriented Design for Implementation\n",
    "\n",
    "### 3.2.1. Utilities\n",
    "- @add_to_class() : We can add specific function to class after the class is created, even after instances are generated!\n",
    "- @class HyperParameters : We can add all arguments in __init__ method to class attributes!\n",
    "\n",
    "## 4.1. Softmax Regression\n",
    "\n",
    "### 4.1.1. Classification\n",
    "- Regression cannot deal with all problem!\n",
    "- In classification problem, we focus on \"which category?\" questions. Indeed, there are cases where more than one label might be true!\n",
    "- There might be problems such as the probability might exceed 1 when it comes to the linear model...\n",
    "- To solve the problems, we should normalize all probabilities between 0 and 1, and the sum of probabilities is always 1. We call the function, which take on the role, Softmax function!\n",
    "- To improve computational efficiency as well, we can vectorize data!\n",
    "\n",
    "### 4.1.2. Loss Function\n",
    "In Softmax regression, we can define loss function by using log-likelihood, where cross-entropy is introduced!\n",
    "\n",
    "## 4.2. The Image Classification Dataset\n",
    "\n",
    "### 4.2.2. Reading a Minibatch\n",
    "We can use built-in data iterator rather than creating on our own, by using iter(data.train_dataloader())!\n",
    "\n",
    "## 4.3. The Base Classification Model\n",
    "\n",
    "### 4.3.2. Accuracy\n",
    "When we define accuracy() function to determine what label is most accurate to given data, we should match data type between y_hat and y because == operator is sensitive to data type!\n",
    "\n",
    "## 4.4. Softmax Regression Implementation from Scratch\n",
    "\n",
    "### 4.4.1. The Softmax\n",
    "If we implement softmax function by scratch, we must indicate that argument X is potentially dangerous when X is too small or too large!\n",
    "\n",
    "### 4.4.3. The Cross-Entropy Loss\n",
    "We can create loss function by introducing cross-entropy loss, which is general in deep learning! (However, this method we are currently implementing is just regression...)\n",
    "\n",
    "## 5.1. Multilayer Perceptrons\n",
    "To jump beyond the limitation of linear model, we can introduce hidden layers between linear layers, and nonlinear activation function such as ReLU, Sigmoid, tanh function!\n",
    "\n",
    "## 5.2. Implementation of Multilayer Perceptrons\n",
    "There are nothing to say as discussions or takeovers... (This chapter are simply talking about how to implement MLP!)\n",
    "\n",
    "## 5.3. Forward Propagation, Backward Propagation, and Computational Graphs\n",
    "\n",
    "### 5.3.1. Forward Propagation\n",
    "In computational graph, we can use forward propagation to compute and save intermediate variables, from input layer to output layer!\n",
    "\n",
    "### 5.3.3. Backpropagation\n",
    "To compute gradient of parameters, we should introduce backpropagation in neural network, using chain rule!\n",
    "\n",
    "### 5.3.4. Training Neural Networks\n",
    "There are dependencies on forward propagation and backward propagation. In other words, the forward propagation computes parameters traversing on the computational graph, and then, the backpropagation computes gradients of parameters to correct them, using chain rule!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
